{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3iCD3piF7H1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12We119hiK1n",
        "outputId": "4e04ac73-c246-49fc-8d34-c1a01b7ed783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (2.3.1+cu118)\n",
            "Requirement already satisfied: torchvision in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (0.18.1+cu118)\n",
            "Requirement already satisfied: torchmetrics in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (1.4.1)\n",
            "Requirement already satisfied: filelock in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==2.3.1 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: numpy in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torchvision) (1.26.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: packaging>17.1 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from torchmetrics) (0.11.6)\n",
            "Requirement already satisfied: setuptools in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/edogawa_congduy/enter/envs/env1/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QgpBIdDla0p5"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torchvision.models as models\n",
        "\n",
        "# # Load the VGG16 model\n",
        "# vgg16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# # Print the model architecture\n",
        "# print(vgg16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GTWI5hqqb8dJ"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/content/cifar100_data'\n",
        "\n",
        "IMAGE_SIZE = 224\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "NUM_EPOCHS = 30\n",
        "NUM_WORKERS = 4\n",
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "CONV_KERNEL = 3\n",
        "CONV_STRIDE = 1\n",
        "CONV_PADDING = 1\n",
        "MP_KERNEL = 2\n",
        "MP_STRIDE = 2\n",
        "MP_PADDING = 0\n",
        "\n",
        "checkpoint_path = 'Build_VGG_Pytorch/saved_models'\n",
        "\n",
        "VGG16_archite = [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yasiFKnnL63w"
      },
      "outputs": [],
      "source": [
        "# def get_args():\n",
        "#     parser = argparse.ArgumentParser(description=\"Train NN model\")\n",
        "#     parser.add_argument(\"--data_path\", \"-d\", type=str, default=\"data/animals\", help=\"path to the dataset\")\n",
        "#     parser.add_argument(\"--batch_size\", \"-b\", type=int, default=16)\n",
        "#     parser.add_argument(\"--image_size\", \"-i\", type=int, default=224)\n",
        "#     parser.add_argument(\"--epochs\", \"-e\", type=int, default=100)\n",
        "#     parser.add_argument(\"--lr\", \"-l\", type=float, default=1e-2)\n",
        "#     parser.add_argument(\"--log_path\", \"-p\", type=str, default=\"tensorboard/animals\")\n",
        "#     parser.add_argument(\"--checkpoint_path\", \"-c\", type=str, default=\"trained_models/animals\")\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E2BBFqMniBFX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, random_split, SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import numpy as np\n",
        "from torchmetrics.classification import MulticlassPrecision, MulticlassRecall, MulticlassAccuracy\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQa1vrtniPyx",
        "outputId": "68346332-db94-4cbe-989f-03033e11deac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA76mhMBDVqU",
        "outputId": "db22a022-90e4-465f-b85c-46413f949011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "def data_loader(data_dir,\n",
        "                batch_size,\n",
        "                random_seed=42,\n",
        "                valid_size=0.1,\n",
        "                shuffle=True,\n",
        "                test=False):\n",
        "\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    # define transforms\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize((227,227)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "    ])\n",
        "\n",
        "    if test:\n",
        "        dataset = datasets.CIFAR100(\n",
        "          root=data_dir, train=False,\n",
        "          download=True, transform=transform,\n",
        "        )\n",
        "\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, shuffle=shuffle\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    # load the dataset\n",
        "    train_dataset = datasets.CIFAR100(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "\n",
        "# CIFAR100 dataset\n",
        "train_loader, valid_loader = data_loader(data_dir='./data',\n",
        "                                         batch_size=BATCH_SIZE)\n",
        "\n",
        "test_loader = data_loader(data_dir='./data',\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              test=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBCxsDK9G23G",
        "outputId": "62014f30-c0bc-42b1-e505-727d73a71162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total step train: 22500\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_loader)\n",
        "print(\"Total step train:\",total_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6BXaDLGyT3bo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def max_pooling_output_size_for_fc(H:int=IMAGE_SIZE, m:int=MP_KERNEL, p:int=MP_PADDING, s:int=MP_STRIDE, type_of_VGG:list=None) -> int:\n",
        "    number_of_max_pooling = sum(1 for element in type_of_VGG if not isinstance(element, int))\n",
        "    for _ in range(number_of_max_pooling):\n",
        "        size = math.floor((H + 2*p - m) / s) + 1\n",
        "    return size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JSKZYguRQZKI"
      },
      "outputs": [],
      "source": [
        "class VGG_nn(nn.Module):\n",
        "  def __init__(self, in_channels=3, num_classes=100):\n",
        "    super(VGG_nn, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.conv_layers = self.create_conv_layers(VGG16_archite)\n",
        "    # self.image_size = max_pooling_output_size_for_fc(H=IMAGE_SIZE, n=len())\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(512*7*7, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(4096, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv_layers(x)\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "  def create_conv_layers(self, archite):\n",
        "    layers = []\n",
        "    in_channels = self.in_channels\n",
        "    for x in archite:\n",
        "      if type(x) == int:\n",
        "        out_channels = x\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(CONV_KERNEL,CONV_KERNEL), stride=(CONV_STRIDE,CONV_STRIDE), padding=(CONV_PADDING,CONV_PADDING)),\n",
        "                   nn.BatchNorm2d(x),\n",
        "                   nn.ReLU()]\n",
        "        in_channels = x\n",
        "      elif x == \"M\":\n",
        "        layers += [nn.MaxPool2d(kernel_size=(MP_KERNEL,MP_KERNEL), stride=(MP_STRIDE,MP_STRIDE))]\n",
        "    return nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nYtK6Sf_YdW6"
      },
      "outputs": [],
      "source": [
        "model = VGG_nn(in_channels=3, num_classes=100).to(device=device)\n",
        "\n",
        "# print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UfQwwM5Ximfr"
      },
      "outputs": [],
      "source": [
        "# defined loss and optimizer function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PT7cqqaxRUi7"
      },
      "outputs": [],
      "source": [
        "# def validate(device, model, val_loader):\n",
        "#     model.eval()\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for images, labels in val_loader:\n",
        "#             images = images.to(device)\n",
        "#             labels = labels.to(device)\n",
        "#             outputs = model(images)\n",
        "\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     accuracy = 100 * correct / total\n",
        "#     print(f\"Accuracy of the network on the {total} validation images: {accuracy:.2f} %\")\n",
        "#     return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ug5rNGK3iY1T"
      },
      "outputs": [],
      "source": [
        "def train(checkpoint_dir):\n",
        "    best_acc = 0.0\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    writer = SummaryWriter('Build_VGG_Pytorch/runs')\n",
        "\n",
        "    precision_metric = MulticlassPrecision(num_classes=10, average='macro').to(device)\n",
        "    recall_metric = MulticlassRecall(num_classes=10, average='macro').to(device)\n",
        "    accuracy_metric = MulticlassAccuracy(num_classes=10).to(device)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        model.train()\n",
        "        progress_bar = tqdm(train_loader, colour='green')\n",
        "        for i, (images, labels) in enumerate(progress_bar):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            output = model(images)\n",
        "            loss = criterion(output, labels)\n",
        "            progress_bar.set_description(f\"Epochs {epoch + 1} / {NUM_EPOCHS} loss: {loss :0.4f}\")\n",
        "            writer.add_scalar('Train/loss', loss, epoch * len(train_loader) + i)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        all_losses = []\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(valid_loader, colour='yellow')\n",
        "            for i, (images, labels) in enumerate(progress_bar):\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                output = model(images)\n",
        "\n",
        "                prediction = torch.argmax(output, dim=1)\n",
        "                loss = criterion(output, labels)\n",
        "                progress_bar.set_description(f\"Epochs {epoch + 1} / {NUM_EPOCHS} loss: {loss :0.4f}\")\n",
        "                all_losses.append(loss.item())\n",
        "                all_labels.extend(labels.tolist())\n",
        "                all_predictions.extend(prediction.tolist())\n",
        "\n",
        "            average_loss = np.mean(all_losses)\n",
        "\n",
        "            # Calculate accuracy, precision, and recall\n",
        "            accuracy = accuracy_metric(torch.tensor(all_predictions).to(device), torch.tensor(all_labels).to(device))\n",
        "            precision = precision_metric(torch.tensor(all_predictions).to(device), torch.tensor(all_labels).to(device))\n",
        "            recall = recall_metric(torch.tensor(all_predictions).to(device), torch.tensor(all_labels).to(device))\n",
        "\n",
        "            print(f\"Precision: {precision.item()} Recall: {recall.item()} Loss: {average_loss} Accuracy: {accuracy.item()}\")\n",
        "            writer.add_scalar(\"Valid/loss\", average_loss, epoch)\n",
        "            writer.add_scalar(\"Valid/accuracy\", accuracy.item(), epoch)\n",
        "            writer.add_scalar(\"Valid/precision\", precision.item(), epoch)\n",
        "            writer.add_scalar(\"Valid/recall\", recall.item(), epoch)\n",
        "\n",
        "            # Save the model checkpoint every epoch\n",
        "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'last.pt'))\n",
        "\n",
        "            # Save the best model\n",
        "            if accuracy.item() > best_acc:\n",
        "                torch.save(model.state_dict(), os.path.join(checkpoint_dir, 'best.pt'))\n",
        "                best_acc = accuracy.item()\n",
        "\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bNTfuyUDq6j7"
      },
      "outputs": [],
      "source": [
        "def testing(device,model,test_loader):\n",
        "  with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "p6-60NzLrFD3"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  print(\"Train\")\n",
        "  train(checkpoint_path)\n",
        "\n",
        "  print(\"Test\")\n",
        "  testing(device,model,test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "dsh8yN9AsDFs",
        "outputId": "63a20d41-91b4-4de9-c54c-42c9bb7f4f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|\u001b[32m          \u001b[0m| 0/22500 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs 1 / 30 loss: 4.5735: 100%|\u001b[32m██████████\u001b[0m| 22500/22500 [1:07:53<00:00,  5.52it/s]\n",
            "Epochs 1 / 30 loss: 4.6137: 100%|\u001b[33m██████████\u001b[0m| 2500/2500 [01:39<00:00, 25.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.009701940231025219 Recall: 0.10000000149011612 Loss: 4.604916426467896 Accuracy: 0.10000000149011612\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epochs 2 / 30 loss: 4.6287: 100%|\u001b[32m██████████\u001b[0m| 22500/22500 [1:07:57<00:00,  5.52it/s]\n",
            "Epochs 2 / 30 loss: 4.5559: 100%|\u001b[33m██████████\u001b[0m| 2500/2500 [01:35<00:00, 26.27it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[10, 10]' is invalid for input of size 114",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[17], line 3\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m   testing(device,model,test_loader)\n",
            "Cell \u001b[0;32mIn[15], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(checkpoint_dir)\u001b[0m\n\u001b[1;32m     42\u001b[0m average_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(all_losses)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Calculate accuracy, precision, and recall\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_metric(torch\u001b[38;5;241m.\u001b[39mtensor(all_predictions)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mtensor(all_labels)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     47\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_metric(torch\u001b[38;5;241m.\u001b[39mtensor(all_predictions)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mtensor(all_labels)\u001b[38;5;241m.\u001b[39mto(device))\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torchmetrics/metric.py:312\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torchmetrics/metric.py:381\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torchmetrics/metric.py:493\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[1;32m    486\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    487\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered different devices in metric calculation (see stacktrace for details).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    488\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m This could be due to the metric class not being on the same device as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m device corresponds to the device of the input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    492\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_on_cpu:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_move_list_states_to_cpu()\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torchmetrics/metric.py:483\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 483\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torchmetrics/classification/stat_scores.py:343\u001b[0m, in \u001b[0;36mMulticlassStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    339\u001b[0m     _multiclass_stat_scores_tensor_validation(\n\u001b[1;32m    340\u001b[0m         preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    341\u001b[0m     )\n\u001b[1;32m    342\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_stat_scores_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m--> 343\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m \u001b[43m_multiclass_stat_scores_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultidim_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state(tp, fp, tn, fn)\n",
            "File \u001b[0;32m~/enter/envs/env1/lib/python3.9/site-packages/torchmetrics/functional/classification/stat_scores.py:414\u001b[0m, in \u001b[0;36m_multiclass_stat_scores_update\u001b[0;34m(preds, target, num_classes, top_k, average, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    412\u001b[0m unique_mapping \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m*\u001b[39m num_classes \u001b[38;5;241m+\u001b[39m preds\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m    413\u001b[0m bins \u001b[38;5;241m=\u001b[39m _bincount(unique_mapping, minlength\u001b[38;5;241m=\u001b[39mnum_classes\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 414\u001b[0m confmat \u001b[38;5;241m=\u001b[39m \u001b[43mbins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m tp \u001b[38;5;241m=\u001b[39m confmat\u001b[38;5;241m.\u001b[39mdiag()\n\u001b[1;32m    416\u001b[0m fp \u001b[38;5;241m=\u001b[39m confmat\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m tp\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 10]' is invalid for input of size 114"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**love u**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
